{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17421920-2a8c-4443-8d7b-35f3fefd0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd259af9-1772-4369-8d35-abb58674117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Computer\n",
      "[nltk_data]     Point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Computer\n",
      "[nltk_data]     Point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Computer\n",
      "[nltk_data]     Point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Computer\n",
      "[nltk_data]     Point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Required for word_tokenize\n",
    "nltk.download('stopwords')  # For stopwords\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642566f8-5291-479e-b07b-05f263d08beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample cleaned text (this would be the result of your text cleaning step)\n",
    "cleaned_text = \"This is a sample text to demonstrate text processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ffd2bb-a26d-44a1-9f7a-282691386de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1b1313-171f-452c-9a34-e9960de40da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812c8639-74c2-4af4-b370-220b0a641198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)  # Tokenize text into words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatize and remove stopwords\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd8847e-d9e8-454b-9c35-a024c00898fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'text', 'demonstrate', 'text', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Process the cleaned text\n",
    "processed_words = preprocess_text(cleaned_text)\n",
    "print(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f5f14-8890-42f2-a0a1-910643ebf85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########3 Method to count the item in list       #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14629c66-ab66-4be7-ace6-fa266e155f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'apple': 3, 'banana': 2, 'orange': 1, 1212: 1})\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "my_list = ['apple', 'banana', 'apple', 'orange', 'banana', 'apple',1212]\n",
    "\n",
    "# Count the occurrences of each item\n",
    "item_counts = Counter(my_list)\n",
    "\n",
    "# Print the counts\n",
    "print(item_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb12e2d-16dc-4b89-90db-7a010571dfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in the list: 7\n"
     ]
    }
   ],
   "source": [
    "# Finding the total number of items in the list\n",
    "total_items = len(my_list)\n",
    "\n",
    "# Print the total\n",
    "print(f\"Total items in the list: {total_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb94ca87-a45e-4b44-8f7e-7ad87ceea2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data using file from folder\n",
    "# def load_words_from_file(file_path):\n",
    "#     \"\"\"Load words from a given text file and return a set of words.\"\"\"\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         # Read lines, strip whitespace, and create a set of words\n",
    "#         words = {line.strip() for line in file if line.strip()}\n",
    "#     return words\n",
    "\n",
    "# # Load positive and negative words\n",
    "# positive_words = load_words_from_file('path/to/file/positive-words.txt')\n",
    "# negative_words = load_words_from_file('path/to/file/negative-word.txt')\n",
    "\n",
    "# # Example of using the sets\n",
    "# print(\"Positive words:\", positive_words)\n",
    "# print(\"Negative words:\", negative_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42162ce3-db8a-4a8b-8e7f-854a797083e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how to give path\n",
    "# The error you're seeing is due to Python treating the backslashes (\\) in your file path as escape characters, like \\U for Unicode.\n",
    "\n",
    "# Solution 1: Use Raw String Literals\n",
    "# One way to handle this is by using a raw string literal by adding an r before the string. This tells Python to treat backslashes as literal characters, not escape sequences.\n",
    "\n",
    "# Here's how you can fix it:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# positive_words = load_words_from_file(r\"D:\\A_Data_Science_Project\\Data Scientist\\URL_based\\positive-word.txt\")\n",
    "# Solution 2: Replace Backslashes with Forward Slashes\n",
    "# Alternatively, you can replace the backslashes (\\) with forward slashes (/). Python can interpret file paths with forward slashes even on Windows.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# positive_words = load_words_from_file(\"D:/A_Data_Science_Project/Data Scientist/URL_based/positive-word.txt\")\n",
    "# Solution 3: Double the Backslashes\n",
    "# You can also escape the backslashes by doubling them (\\\\):\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# positive_words = load_words_from_file(\"D:\\\\A_Data_Science_Project\\\\Data Scientist\\\\URL_based\\\\positive-word.txt\")\n",
    "# Any of these solutions should resolve the SyntaxError you're seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c43cf-b977-4ada-a2cc-bcd10de04832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
